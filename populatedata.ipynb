{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# code updated on 3 October 2024\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "doc = './HP-datasheet.pdf'\n",
    "\n",
    "# takes 1 min on Colab\n",
    "loader = UnstructuredPDFLoader(file_path=doc,\n",
    "                               strategy='hi_res',\n",
    "                               extract_images_in_pdf=True,\n",
    "                               infer_table_structure=True,\n",
    "                               chunking_strategy=\"by_title\", # section-based chunking\n",
    "                               max_characters=4000, # max size of chunks\n",
    "                               new_after_n_chars=4000, # preferred size of chunks\n",
    "                               combine_text_under_n_chars=2000, # smaller chunks < 2000 chars will be combined into a larger chunk\n",
    "                               mode='elements',\n",
    "                               image_output_dir_path='./figures')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize the model\n",
    "chatgpt = Ollama(model=\"llama3.1:8b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "tables = []\n",
    "\n",
    "for doc in data:\n",
    "    if doc.metadata['category'] == 'Table':\n",
    "        tables.append(doc)\n",
    "    elif doc.metadata['category'] == 'CompositeElement':\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text particularly for semantic retrieval.\n",
    "These summaries will be embedded and used to retrieve the raw text or table elements\n",
    "Give a detailed summary of the table or text below that is well optimized for retrieval.\n",
    "For any tables also add in a one line description of what the table is about besides the summary.\n",
    "Do not add additional words like Summary: etc.\n",
    "\n",
    "Table or text chunk:\n",
    "{element}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "summarize_chain = (\n",
    "                    {\"element\": RunnablePassthrough()}\n",
    "                      |\n",
    "                    prompt\n",
    "                      |\n",
    "                    chatgpt\n",
    "                      |\n",
    "                    StrOutputParser() # extracts the response as text and returns it as a string\n",
    ")\n",
    "\n",
    "# Initialize empty summaries\n",
    "text_summaries = []\n",
    "table_summaries = []\n",
    "\n",
    "text_docs = [doc.page_content for doc in docs]\n",
    "table_docs = [table.page_content for table in tables]\n",
    "\n",
    "text_summaries = summarize_chain.batch(text_docs, {\"max_concurrency\": 5})\n",
    "table_summaries = summarize_chain.batch(table_docs, {\"max_concurrency\": 5})\n",
    "\n",
    "len(text_summaries), len(table_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 44.594937801361084 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import os\n",
    "import time\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "   \n",
    "    chat = ChatOllama(model=\"llava-llama3\", temperature=0)\n",
    "    \n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval.\n",
    "                Remember these images could potentially contain graphs, charts or tables also.\n",
    "                These summaries will be embedded and used to retrieve the raw image for question answering.\n",
    "                Give a detailed summary of the image that is well optimized for retrieval.\n",
    "                Do not add additional words like Summary: etc.\n",
    "             \"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "IMG_PATH = './figures'\n",
    "start_time = time.time()\n",
    "imgs_base64, image_summaries = generate_img_summaries(IMG_PATH)\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DAVID\\AppData\\Local\\Temp\\ipykernel_13576\\1937082870.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  chroma_db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Code from Langchain for calling chromadb\n",
    "# Do not run for the first time\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(\"private_rag\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"private_rag\",\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DAVID\\AppData\\Local\\Temp\\ipykernel_33140\\3635463687.py:6: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  chroma_db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# This code can be run for the first time to populate chromadb\n",
    "# DO NOT RUN ON THE SECOND OR SUBSEQUENT TIME IF PERSISTED\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"private_rag\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./privatedb\",\n",
    "    # collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code for the first time\n",
    "\n",
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain_community.utilities.redis import get_client\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    docstore, vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    ## THE FOLLOWING SUPPOSED CAN BE COMMENTED OUT AND LEFT return retriever only if I have persisted vectorstore and docstore previously\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# Initialize the storage layer - to store raw images, text and tables\n",
    "client = get_client('redis://localhost:6379')\n",
    "redis_store = RedisStore(client=client) # you can use filestore, memorystory, any other DB store also\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector = create_multi_vector_retriever(\n",
    "    redis_store,\n",
    "    chroma_db,\n",
    "    text_summaries,\n",
    "    text_docs,\n",
    "    table_summaries,\n",
    "    table_docs,\n",
    "    image_summaries,\n",
    "    imgs_base64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal RAG\n",
    "\n",
    "from IPython.display import HTML, display, Image\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Decode the base64 string\n",
    "    img_data = base64.b64decode(img_base64)\n",
    "    # Create a BytesIO object\n",
    "    img_buffer = BytesIO(img_data)\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(img_buffer)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content.decode('utf-8')\n",
    "        else:\n",
    "            doc = doc.decode('utf-8')\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain_community.utilities.redis import get_client\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    docstore, vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    ## THE FOLLOWING SUPPOSED CAN BE COMMENTED OUT AND LEFT return retriever only if I have persisted vectorstore and docstore previously\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# Initialize the storage layer - to store raw images, text and tables\n",
    "client = get_client('redis://localhost:6379')\n",
    "redis_store = RedisStore(client=client) # you can use filestore, memorystory, any other DB store also\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector = create_multi_vector_retriever(\n",
    "    redis_store,\n",
    "    chroma_db,\n",
    "    text_summaries,\n",
    "    text_docs,\n",
    "    table_summaries,\n",
    "    table_docs,\n",
    "    image_summaries,\n",
    "    imgs_base64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def multimodal_prompt_function(data_dict):\n",
    "    \"\"\"\n",
    "    Create a multimodal prompt with both text and image context.\n",
    "\n",
    "    This function formats the provided context from `data_dict`, which contains\n",
    "    text, tables, and base64-encoded images. It joins the text (with table) portions\n",
    "    and prepares the image(s) in a base64-encoded format to be included in a message.\n",
    "\n",
    "    The formatted text and images (context) along with the user question are used to\n",
    "    construct a prompt for multimodal model.\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = \"\"\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = f\"Image: data:image/jpeg;base64,{image}\\n\"\n",
    "            messages += image_message\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = f\"\"\"\n",
    "        You are an pre-sales specialist well-versed with the company products based on the datasheet and brochure available and\n",
    "        are competent in matching the requirements from potential user or clients with the available product specification. \n",
    "        You will be given context information below which will be a mix of text, tables, and images usually of charts or graphs.\n",
    "        Use this information to provide answers related to the question which should be related to minimum specification required by the user.\n",
    "        Your answer should specify a value or text extracted from given document only. \n",
    "        DO NOT include preamble or your own analysis. \n",
    "        DO NOT give me the sources where you obtain your information or conclusion. \n",
    "        DO NOT make up answers, use the provided context documents below and answer the question to the best of your ability. \n",
    "\n",
    "        #Example 1: \n",
    "        User question: 'Weight: Maximum 3kg'\n",
    "        Your answer: 'Minimum weight starts at 1.21kg'\n",
    "        \n",
    "        #Example 2:\n",
    "        User question: 'Accessories: laptop bag'\n",
    "        answer: 'Accessories: not provided'\n",
    "\n",
    "        #Example 3:\n",
    "        User question: 'Application Software: Microsoft Office'\n",
    "        answer: 'Application Software: not provided'\n",
    "\n",
    "        Let's start:\n",
    "\n",
    "        User question:\n",
    "        {data_dict['question']}\n",
    "\n",
    "        Context documents:\n",
    "        {formatted_texts}\n",
    "\n",
    "        Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine text and image message\n",
    "    messages += text_message\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "# Create RAG chain\n",
    "multimodal_rag = (\n",
    "        {\n",
    "            \"context\": itemgetter('context'),\n",
    "            \"question\": itemgetter('input'),\n",
    "        }\n",
    "            |\n",
    "        RunnableLambda(multimodal_prompt_function)\n",
    "            |\n",
    "        chatgpt\n",
    "            |\n",
    "        StrOutputParser()\n",
    ")\n",
    "\n",
    "# Pass input query to retriever and get context document elements\n",
    "retrieve_docs = (itemgetter('input')\n",
    "                    |\n",
    "                retriever_multi_vector\n",
    "                    |\n",
    "                RunnableLambda(split_image_text_types))\n",
    "\n",
    "# Below, we chain `.assign` calls. This takes a dict and successively\n",
    "# adds keys-- \"context\" and \"answer\"-- where the value for each key\n",
    "# is determined by a Runnable (function or chain executing at runtime).\n",
    "# This helps in also having the retrieved context along with the answer generated by GPT-4\n",
    "multimodal_rag_w_sources = (RunnablePassthrough.assign(context=retrieve_docs)\n",
    "                                               .assign(answer=multimodal_rag)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overtaken by the same function below. \n",
    "## The function below specifically produce json output. \n",
    "\n",
    "def multimodal_rag_qa(query):\n",
    "    response = multimodal_rag_w_sources.invoke({'input': query})\n",
    "    print(response['answer']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"You are a fair evaluator language model.\n",
    "\n",
    "You will be given an instruction, a response to evaluate, a reference answer that gets a score of 3, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 3. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 3}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "5. Do not score conciseness: a correct answer that covers the question should receive max score, even if it contains additional useless information.\n",
    "\n",
    "The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "Response to evaluate:\n",
    "{response}\n",
    "\n",
    "Reference Answer (Score 3):\n",
    "{reference_answer}\n",
    "\n",
    "Score Rubrics:\n",
    "[Is the response complete, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incomplete, inaccurate, and/or not factual.\n",
    "Score 2: The response is somewhat complete, accurate, and/or factual.\n",
    "Score 3: The response is completely complete, accurate, and/or factual.\n",
    "\n",
    "Feedback:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "eval_prompt = ChatPromptTemplate.from_template(EVALUATION_PROMPT)\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize the model\n",
    "evaluation_llm = Ollama(model=\"llama3.1:8b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('Sample-TenderDoc.xlsx')\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_response(instruction, response, reference_answer):\n",
    "    EVALUATION_PROMPT = \"\"\"You are a fair evaluator language model.\n",
    "\n",
    "    You will be given an instruction, a response to evaluate, a reference answer that gets a score of 3, and a score rubric representing a evaluation criteria are given.\n",
    "    1. Write a score that is an integer between 1 and 3. You should refer to the score rubric.\n",
    "    2. The output format should look as follows: \"{{an integer number between 1 and 3}}\"\n",
    "    3. Please do not generate any other opening, closing, and explanations.\n",
    "    4. Failure to give a numeric value in your output is unacceptable and will result in termination.\n",
    "     \n",
    "    The instruction to evaluate:\n",
    "    {instruction}\n",
    "\n",
    "    Response to evaluate:\n",
    "    {response}\n",
    "\n",
    "    Reference Answer (Score 3):\n",
    "    {reference_answer}\n",
    "\n",
    "    Score Rubrics:\n",
    "    [Is the response complete, accurate, and factual based on the reference answer?]\n",
    "    Score 1: The response is completely incomplete, inaccurate, and/or not factual.\n",
    "    Score 2: The response is somewhat complete, accurate, and/or factual.\n",
    "    Score 3: The response is completely complete, accurate, and/or factual.\n",
    "    \"\"\"\n",
    "\n",
    "    eval_prompt = ChatPromptTemplate.from_template(EVALUATION_PROMPT)\n",
    "    prompt = eval_prompt.format(instruction=instruction, response=response, reference_answer=reference_answer)\n",
    "    \n",
    "    evaluation = evaluation_llm(prompt)\n",
    "    parsed_output = StrOutputParser().parse(evaluation).strip()\n",
    "    \n",
    "    # Extract the score from the parsed output\n",
    "    try:\n",
    "        score = int(parsed_output.split()[0])\n",
    "    except (ValueError, IndexError) as e:\n",
    "        raise ValueError(f\"Invalid score value: {parsed_output}\") from e\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Iterate over the rows and generate responses and evaluations\n",
    "generated_responses = []\n",
    "evaluation_scores = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    min_spec = row['Minimum Specification']\n",
    "    expected_response = row['Expected Response']\n",
    "    query = f\"Based on the datasheet, what is the closest specification that could meet or exceed this technical requirement: {min_spec} ?\"\n",
    "\n",
    "    # Generate response\n",
    "    generated_response = multimodal_rag_qa(query)\n",
    "    if generated_response.startswith('Generated response for:'):\n",
    "        generated_response = generated_response[len('Generated response for:'):].strip()\n",
    "    generated_responses.append(generated_response)\n",
    "    \n",
    "    # Evaluate response\n",
    "    try:\n",
    "        score = evaluate_response(query, generated_response, expected_response)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error evaluating response for row {index}: {e}\")\n",
    "        score = None  # or some default value\n",
    "    evaluation_scores.append(score)\n",
    "\n",
    "# Add the new columns to the DataFrame\n",
    "df['Generated Response'] = generated_responses\n",
    "df['Evaluation Score'] = evaluation_scores\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df.to_excel('Updated-Sample-TenderDoc.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
